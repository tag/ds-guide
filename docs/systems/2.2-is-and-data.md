# Data & Information Systems
Data are used in many contexts. In designing and using data systems, it's important to consider how data are generated, how data will be used, and how data will be stored.

## The lifecycle of data
Data have lifecycles. Data might be generated by sensors or derived from the output of other systems. When aggregated or interpreted, data becomes information that can inform decision making. Decisions drive transactional systems, which in turn generate more data. Although data might be processed in real time ("data in use"), it is most often sent across a network ("data in motion") and stored in an archival system, such as a data warehouse ("data at rest").

In his book Data Management [^watson-dm], Richard Watson describes how data are generated by transactions and sensors. Every business transaction generates new data. Sensors are everywhere. Our phones have many sensors, including a barometer and a GPS. Phones and smart watches include accelerometers, which are used to sense direction of travel and count steps. Most of the web sites we visit include tracking scripts that store which browser I'm using, and watch which parts of the page we look at, how long we are active on a page, where we arrived from and how we left the page. Each of us generate kilobytes (possibly megabytes) of data each day that are tracked and stored.

<!-- <figure markdown="span"> -->

![Transactions → ][watson-is-cycle]
<figcaption>**Figure 2-1:** The Information Systems Cycle[^watson-dm-adapted]</figcaption>
<!-- </figure> -->

[watson-is-cycle]: ../img/data-cycle.png "Figure 2–1: The Information Systems Cycle"
[^watson-dm]: Watson, Richard (2016-10-06). _Data Management (6th ed.)_. eGreen Press.
[^watson-dm-adapted]: Adapted from Watson (2016)[^watson-dm]

Data are created by automatic systems too. If I were running a website for my company, I might want to have an automated service running that checks every few minutes whether my website was working, and generate alerts. Each check would be logged so I could audit the data later to ensure everything was running as expected.

Transactions, sensors, and automated processes generate massive amounts of data each second. These data are generated by information systems, flow through information systems networks, and eventually "rest" in a particular type of information system called _**data warehouses**_. While in motion, data need to be _**serialized**_, meaning translated in a way that can be shared by different systems or stored and reconstructed later.

A system might not store all of the data that's been generated. Consider a word processing application. While you use it, the application keeps a list of all of the actions you've taken in editing your document. You can move back and forward through this list with the "Undo" and "Redo" commands. However, when you save and close your document, this edit history isn't usually saved and available for you the next time you open it. All of that extra data has been discarded. Another example might be a security camera, which  appears to record in real time, but in reality only stores data when it detects movement, or only stores one frame per second. Both of these strategies attempt to identify which data are important and which are not.

Data in warehouses tends to be historical, and thus unlikely to change.
<!-- More here on databases, file systems -->

*[GPS]: Global Positioning System


## Three states of data
As mentioned previously, data are considered to be in one of three states at any
given time and place. Of course, copies of the same data might be in all three
states (data in active memory is a copy of data on a hard drive), but each copy
is only in one state at any given time. The three states are _data in motion_ as
it moves across a network, _data at rest_ as it's stored in a persistent medium,
and _data in use_, which exists in a transient medium, such as an application's
active memory allocation.

These three states of data are a useful framework for considering other aspects,
such as security[^cernegie], serialization, or what infrastructure is required.
To use security as an example, data at rest may be encrypted with security keys
that are reused, whereas data in motion, when encrypted, typically uses
dynamically generated temporary keys that frequently change and are not stored.
Data in motion is often serialized in a human readable way (before encryption),
to facilitate debugging and inspection by developers. XML and JSON are common
serialization formats for web applications, but might not necessarily be used in
persistent data stores.

[^carnegie]: ["Moving the Encryption Policy Conversation Forward"][carnegie-report] Encryption Working Group, Carnegie Endowment for International Peace and Princeton University, September, 2019. See also, Bruce Schneier, "More on Law Enforcement Backdoor Demands"](https://www.schneier.com/blog/archives/2019/09/more_on_law_enf.html), September 11, 2019

[carnegie-report]: https://carnegieendowment.org/2019/09/10/moving-encryption-policy-conversation-forward-pub-79573

<!--
MORE! https://www.datamotion.com/2015/12/best-practices-securing-data-at-rest-in-use-and-in-motion/
https://en.wikipedia.org/wiki/Data_at_rest#Alternative_definition -->

<!--
### Data in motion
encryption, endpoint security; network might be untrusted (the Internet)

### Data at rest
Data storage systems have some constraints or trade-offs that have to be considered. The first is a tradeoff between capacity and speed. This is most easily demonstrated by considering the price of random access memory (RAM), as compared with a hard drive, or the price of a solid-state hard drive (SSD) to a traditional spinning platter hard drive. In each case, as the speed goes up the prices goes up and the storage capacity goes down. RAM is very fast, but expensive relative to other options. The solid state drive is growing in popularity as prices are coming down, but SSDs are still significantly more expensive that older spinning platter drives. Data centers, often use magnetic tape drives for backup systems, which require massive storage but don't require blazing speed. No matter how we store data, the faster it is to retrieve and process it, the more expensive the storage solution will be. Eventually, there will be too much data in a system to keep it on one computer, which requires additional trade-offs.


### Data in use -->


## CAP Theorem

_**CAP Theorem**_, also called Brewer’s Theorem, states that at best, a distributed system can provide only two of the three guarantees:

  * **Consistency:**	All nodes see the same data at the same time
 	* **Availability:**	Every request receives a (successful) response
  * **Partition tolerance:** The system continues to operate despite arbitrary partitioning due to  network failures

These properties are not, in practice perfectly exclusive, although it is not
possible to have both perfect availability and perfect consistency in a
partitioned system. As Brewer has said, "Although designers still need to choose between consistency and availability when partitions are present, there is an incredible range of flexibility for handling partitions and recovering from them."[^brewer]

<!-- TODO: More on CAP and the tradeoffs of different systems -->

 <!-- Problems with data management systems

 Lack of redundancy
 Lack of data control
 Poor interface
 Lack of access/lack of security
 Delays
 Lack of reality
 Diversity of systems/silos
 Lack of data integration
 Volume of data -->


 <!-- TODO:  Data silos are revealed as an organization grows. -->

<!-- ## Common formats for data

## Data storage

## Data in transit

### XML

### JSON

### Other formats
CSV ("Comma-Separated Values"), YAML ("Yet Another Markup Language") -->

*[YAML]: Yet Another Markup Language
*[XML]: eXtensible Markup Language
*[JSON]: Javascript Object Notation

## Additional Reading

* ["Serialization"](https://en.wikipedia.org/wiki/Serialization), _Wikipedia_
*  Eric Brewer, ["CAP twelve years later: How the 'rules' have changed"][brewer-cap], Computer, Volume 45, Issue 2 (2012), pg. 23–29. doi:10.1109/MC.2012.37.

[^brewer]: Eric Brewer, ["CAP twelve years later: How the 'rules' have changed"][brewer-cap], Computer, Volume 45, Issue 2 (2012), pg. 23–29. doi:10.1109/MC.2012.37.

[brewer-cap]: http://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed

## Review

1. Explain data in motion, data in use, and data at rest.
2. Explain CAP Theorem.

## Exercises

1. Describe a university class using XML. Include multiple students per class section, and multiple class sections for a course.
2. Describe a university class using JSON. Include multiple students per class section, and multiple class sections for a course.
3. Choose a relational database (e.g., MySQL) and a non-relational database
   (e.g., MongoDb). How do these products handle the constraints of CAP Theorem?
